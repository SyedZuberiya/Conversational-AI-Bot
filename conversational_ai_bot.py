# -*- coding: utf-8 -*-
"""Conversational AI Bot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oAMTwVfswoqivm4Pu00pHpzry3rnRodT
"""

pip install pymupdf langchain sentence-transformers faiss-cpu transformers torch

!pip install -U langchain-community

import os
import fitz  # PyMuPDF
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import TokenTextSplitter
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline


def load_pdf_text(pdf_path: str) -> str:
    """Extract all text from the PDF file."""
    doc = fitz.open(pdf_path)
    full_text = ""
    for page in doc:
        full_text += page.get_text()
    return full_text


def create_vector_store_from_text(text: str) -> FAISS:
    """Split text into chunks by tokens and create FAISS vectorstore."""
    splitter = TokenTextSplitter(chunk_size=400, chunk_overlap=50)
    chunks = splitter.split_text(text)
    docs = [Document(page_content=chunk) for chunk in chunks]

    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore


def create_qa_chain(vectorstore: FAISS) -> RetrievalQA:
    """Create RetrievalQA chain using Flan-T5-base model and FAISS retriever."""
    model_name = "google/flan-t5-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    device = 0 if torch.cuda.is_available() else -1

    pipe = pipeline(
        "text2text-generation",
        model=model,
        tokenizer=tokenizer,
        max_length=256,
        do_sample=True,
        temperature=0.7,
        device=device,
    )

    llm = HuggingFacePipeline(pipeline=pipe)

    # Retrieve only top 1 chunk for best focused context
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1})

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=False,
    )
    return qa_chain


if __name__ == "__main__":
    pdf_path = "/content/Syeda_Zuberiya_6305553234.pdf"  # <-- Update with your PDF path

    if not os.path.isfile(pdf_path):
        print(f"Error: PDF file '{pdf_path}' not found. Please check the path.")
        exit(1)

    print("Loading PDF and extracting text...")
    text = load_pdf_text(pdf_path)

    print("Creating vector store (embedding chunks)...")
    vectorstore = create_vector_store_from_text(text)

    print("Creating QA chain...")
    qa = create_qa_chain(vectorstore)

    print("\nðŸ¤– Ask questions (type 'exit' to quit):")
    while True:
        query = input("You: ").strip()
        if query.lower() in ["exit", "quit"]:
            print("Exiting chatbot.")
            break

        try:
            answer = qa.invoke({"query": query})
            print("Bot:", answer["result"])
        except Exception as e:
            print(f"Error while processing query: {e}")